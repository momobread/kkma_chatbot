# -*- coding: utf-8 -*-

"""chatbot2_kor

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VAmDLPcneAcGuF-h6h8gXuSW0gxrwv8L

"""
#한국어 버젼입니다



import os
from gensim.models import doc2vec
from gensim.models.doc2vec import TaggedDocument

#구글그라이브 연동
#from google.colab import drive
#drive.mount('/content/drive/')

import jpype
from konlpy.tag import Kkma

kkma = Kkma()
def tokenizer_kkma(doc):
  jpype.attachThreadToJVM()
  token_doc = ['/'.join(word) for word in kkma.pos(doc)]
  return token_doc

import pandas as pd #파이썬에서 엑셀처럼 데이터 관리

#드라이브 내에서 csv경로 설정
#df_faq = pd.read_csv(os.path.join('data','/content/drive/My Drive/data/kor_elec_faq2.csv'),encoding='CP949')
#df_faq = pd.read_csv(os.path.join('data','/content/drive/My Drive/data/kor_school_faq.csv'),encoding='CP949')

#로컬에서 불러오기 위한 라이브러리
#from google.colab import files
#uploaded = files.upload()


#로컬에서 불러오기
import io
import pandas as pd

#df_faq = pd.read_csv('kor_school_faq.csv',encoding='CP949')
global df_faq 
df_faq= pd.read_csv('kor_elec_faq.csv',encoding='UTF-8')
df_faq
df_faq[['제목','내용']]

#질문내용/답변내용
#[index,형태소분석(질문),답]
pre_faqs = []
for i in range(len(df_faq)):
    pre_faqs.append([i,
                     tokenizer_kkma(df_faq['제목'][i]),
                     df_faq['내용'][i]]
                    )
pre_faqs


#list 첫번째는 질문들 두번째는 인덱스(질문들 다 쪼개기)
#token_faqs = [(tokenizer_kkma(row[1]),row[0]) for row in faqs]

tagged_faqs = [TaggedDocument(c,[d]) for d,c,a in pre_faqs]
#tagged_faqs = [TaggedDocument(c,[d]) for d,c in enumerate(pre_faqs)]
#tagged_faqs = [TaggedDocument(words=str(pre_faqs), tags=[str(i)]) for i, pre_faqs in pre_faqs]

#d는 인덱스 c는 질문 a는 답변
tagged_faqs #태그값으로 구분하기 때문에 태그값을 꼭 넣어줘야 된다

#모델 만들기
import multiprocessing
cores = multiprocessing.cpu_count()
d2v_faqs = doc2vec.Doc2Vec(
    vector_size = 20, ##
    alpha = 0.025,  #런닝메이트
    min_alpha = 0.025,
    hs =1,
    negative = 0, ##
    dm = 0,
    dbow_words = 1,
    min_count =1, ##
    workers = cores,
    seed = 0, #초기 백터값 고정 가능
    #epochs=20


)
d2v_faqs.build_vocab(tagged_faqs) # 단어사전 만들기

#학습시키기
for epoch in range(100):
  d2v_faqs.train(
      tagged_faqs,
      total_examples= d2v_faqs.corpus_count,
      epochs = d2v_faqs.epochs

  )

d2v_faqs.alpha -= 0.0025 #갈수록 학습률을 줄인다 => 속도향상을 위해
d2v_faqs.min_alpha = d2v_faqs.alpha #


#답변

def faq_answer(input):
    try:
        #들어온 질문을 백터화하기
        #test_string ="아파트 관리사무소입니다. 종합계약과 단일계약을 비교해보고 싶습니다."
        test_string = input
        #"회원가입 시 자꾸 이상한 메시지가 뜹니다."
        token_test = tokenizer_kkma(test_string)
        #print(type(input))
        #print(token_test)
        

        predict_vector = d2v_faqs.infer_vector(token_test)
        predict_vector


        #model.docvecs.most_similar(positive=[vec], topn=10)[1:]

        #가장 유사한 값을 찾자
        result = d2v_faqs.docvecs.most_similar(positive=[predict_vector],topn=5)
        #result = d2v_faqs.docvecs.most_similar(predict_vector,topn=5)
        print (result)
        print(result[0][0])
        print(df_faq)
        print(type(df_faq))
        print("//////")
        print(df_faq.head())
        print(df_faq.columns)
        print(df_faq.index)
        print("//////")
        print(df_faq.loc[2,'내용'])
        print("///////")
        print(type(result[0][0]))
        new_index = int(result[0][0])
        print(df_faq.loc[new_index,'내용'])

            #    for i in range(5):
            #        print("{}순위 {}번문장 {}".format(i+1,result[i][0],df_faq["답변내용"][result[i][0]]))
                                                    #result[i][0] ==태그값
    
            #df_faq.loc[[result[0][0]],['내용']]

        return df_faq.loc[new_index,'내용']
    except Exception as e:
        print("오류",str(e))
    #df_faq[['제목','내용']]
    #return result[0][0]

  



#  try:
#        # 함수의 본래 동작
#        response = "정상적인 응답"
#        return response
#    except Exception as e:
#        # 예외가 발생한 경우 예외 처리
#        print("오류 발생:", str(e))
#        response = "오류가 발생했습니다."
#        return response